{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG19_Crack_Detection&Localization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "540dfba34ded419c8257a4d6f9128293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d5b6ff5322d41b0969551136ed4558d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_511ddf3a93b44b46ad37737e40a627d3",
              "IPY_MODEL_6f6e6336979f4ea386fde7d875eec600"
            ]
          }
        },
        "6d5b6ff5322d41b0969551136ed4558d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "511ddf3a93b44b46ad37737e40a627d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cba0f7dd54144cb08eb5a06f64b64c49",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5a7de2ee1f6437aa35a528b04a4721f"
          }
        },
        "6f6e6336979f4ea386fde7d875eec600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de279b50149742e2b559530dcd746882",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:09&lt;00:00, 59.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_530abd690b61444dbab0609083959055"
          }
        },
        "cba0f7dd54144cb08eb5a06f64b64c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5a7de2ee1f6437aa35a528b04a4721f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de279b50149742e2b559530dcd746882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "530abd690b61444dbab0609083959055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4xafRJkxMLG"
      },
      "source": [
        "#https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from torchvision.models import vgg19\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mNbLkod_ZT",
        "outputId": "bdb1ef93-45bf-493b-c858-c8968ed4673c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKSDXAD8eCBD",
        "outputId": "13fc1ddf-408e-4039-9082-cac4f8b94302"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Machine_Learning/Crack_Detection') \n",
        "!pwd\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Machine_Learning/Crack_Detection\n",
            " Autoencoders_Conv2d.ipynb   map.jpg\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                       Neg_01249.tif\n",
            " \u001b[01;34mdata_2\u001b[0m/                     Pos_00045.tif\n",
            " \u001b[01;34mGAN\u001b[0m/                        Pos_00100.tif\n",
            " map_1.jpg                  'VGG19_Crack_Detection&Localization.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aal2ByvgdFx2",
        "outputId": "e85619e9-b4fc-4c07-b513-93bd140c1597"
      },
      "source": [
        "#https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d\n",
        "from skimage.color import gray2rgb\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.imgs_path = \"data_2/\"#\"Crack_Detection/\"\n",
        "        file_list = glob.glob(self.imgs_path + \"*\")\n",
        "        print(file_list)\n",
        "        self.data = []\n",
        "        for class_path in file_list:\n",
        "            class_name = class_path.split(\"/\")[-1]\n",
        "            for img_path in glob.glob(class_path + \"/*.tif\"):#tif\n",
        "                self.data.append([img_path, class_name])\n",
        "        print(len(self.data), self.data[:10])\n",
        "        self.class_map = {\"Negative\" : 0, \"Positive\": 1}\n",
        "        self.img_dim = (224, 224)   \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "        # converting the type of pixel to float 32\n",
        "        #img = img.astype('float32')\n",
        "        # normalizing the pixel values\n",
        "        #img /= 255.0\n",
        "        #\n",
        "        img = cv2.resize(img, self.img_dim)\n",
        "        class_id = self.class_map[class_name]\n",
        "        img_tensor = torch.from_numpy(gray2rgb(img))\n",
        "        img_tensor = img_tensor.permute(2, 0, 1)\n",
        "        class_id = torch.tensor([class_id])\n",
        "        #return img_tensor, class_id\n",
        "        return img_tensor.float(), class_id#.float()\n",
        "\n",
        "dataset = CustomDataset()\n",
        "batch_size=64\n",
        "\n",
        "trainloader = DataLoader(dataset, batch_size, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data_2/Positive', 'data_2/Negative']\n",
            "384 [['data_2/Positive/Pos_00045.tif', 'Positive'], ['data_2/Positive/Pos_00002.tif', 'Positive'], ['data_2/Positive/Pos_00049.tif', 'Positive'], ['data_2/Positive/Pos_00020.tif', 'Positive'], ['data_2/Positive/Pos_00017.tif', 'Positive'], ['data_2/Positive/Pos_00001.tif', 'Positive'], ['data_2/Positive/Pos_00024.tif', 'Positive'], ['data_2/Positive/Pos_00018.tif', 'Positive'], ['data_2/Positive/Pos_00041.tif', 'Positive'], ['data_2/Positive/Pos_00012.tif', 'Positive']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO5pOKLTxP2A"
      },
      "source": [
        "class VGG(nn.ModuleList):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        #self.bs = batch_size\n",
        "        \n",
        "        # get the pretrained VGG19 network\n",
        "        self.vgg = vgg19(pretrained=True)\n",
        "        \n",
        "        # disect the network to access its last convolutional layer\n",
        "        self.features_conv = self.vgg.features[:36]\n",
        "        \n",
        "        # get the max pool of the features stem\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        \n",
        "        # get the classifier of the vgg19\n",
        "        self.classifier = nn.Sequential(nn.Linear(in_features=7*7*512, out_features=1000), nn.ReLU(True), nn.Dropout(0.5), nn.Linear(in_features=1000, out_features=2), nn.Sigmoid()) #, nn.Sigmoid()#self.vgg.classifier\n",
        "        #self.classifier = nn.Sequential(nn.Linear(in_features=2, out_features=1), nn.Sigmoid()) ##self.vgg.classifier\n",
        "        \n",
        "        # placeholder for the gradients\n",
        "        self.gradients = None\n",
        "    \n",
        "    # hook for the gradients of the activations\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features_conv(x)\n",
        "        \n",
        "        # register the hook\n",
        "        h = x.register_hook(self.activations_hook)\n",
        "        \n",
        "        # apply the remaining pooling\n",
        "        x = self.max_pool(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    # method for the gradient extraction\n",
        "    def get_activations_gradient(self):\n",
        "        return self.gradients\n",
        "    \n",
        "    # method for the activation exctraction\n",
        "    def get_activations(self, x):\n",
        "       return self.features_conv(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "540dfba34ded419c8257a4d6f9128293",
            "6d5b6ff5322d41b0969551136ed4558d",
            "511ddf3a93b44b46ad37737e40a627d3",
            "6f6e6336979f4ea386fde7d875eec600",
            "cba0f7dd54144cb08eb5a06f64b64c49",
            "b5a7de2ee1f6437aa35a528b04a4721f",
            "de279b50149742e2b559530dcd746882",
            "530abd690b61444dbab0609083959055"
          ]
        },
        "id": "I3Ynpf34gUrp",
        "outputId": "c2579772-a355-4dfd-fbb1-869b1a0ad11d"
      },
      "source": [
        "#model.compile(optimizer= Adam(lr=0.0001), loss = \"BinaryCrossentropy\", metrics = ['acc'])\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda:0'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "print(device)\n",
        "\n",
        "# initialize the VGG model\n",
        "vgg = VGG()\n",
        "vgg.to(device)\n",
        "print(vgg)\n",
        "from torchsummary import summary\n",
        "summary(vgg,(3,224,224))\n",
        "\n",
        "#Loss function\n",
        "criterion = torch.nn.BCELoss()#BCEWithLogitsLoss()#\n",
        "\n",
        "#Optimizer\n",
        "optimizer = torch.optim.SGD(vgg.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "540dfba34ded419c8257a4d6f9128293",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "VGG(\n",
            "  (vgg): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (17): ReLU(inplace=True)\n",
            "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (24): ReLU(inplace=True)\n",
            "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (26): ReLU(inplace=True)\n",
            "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (31): ReLU(inplace=True)\n",
            "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (33): ReLU(inplace=True)\n",
            "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (35): ReLU(inplace=True)\n",
            "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (features_conv): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "  )\n",
            "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=1000, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=1000, out_features=2, bias=True)\n",
            "    (4): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "            Conv2d-2         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-3         [-1, 64, 224, 224]               0\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "            Conv2d-5         [-1, 64, 224, 224]          36,928\n",
            "            Conv2d-6         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-7         [-1, 64, 224, 224]               0\n",
            "              ReLU-8         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-9         [-1, 64, 112, 112]               0\n",
            "        MaxPool2d-10         [-1, 64, 112, 112]               0\n",
            "           Conv2d-11        [-1, 128, 112, 112]          73,856\n",
            "           Conv2d-12        [-1, 128, 112, 112]          73,856\n",
            "             ReLU-13        [-1, 128, 112, 112]               0\n",
            "             ReLU-14        [-1, 128, 112, 112]               0\n",
            "           Conv2d-15        [-1, 128, 112, 112]         147,584\n",
            "           Conv2d-16        [-1, 128, 112, 112]         147,584\n",
            "             ReLU-17        [-1, 128, 112, 112]               0\n",
            "             ReLU-18        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-19          [-1, 128, 56, 56]               0\n",
            "        MaxPool2d-20          [-1, 128, 56, 56]               0\n",
            "           Conv2d-21          [-1, 256, 56, 56]         295,168\n",
            "           Conv2d-22          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-23          [-1, 256, 56, 56]               0\n",
            "             ReLU-24          [-1, 256, 56, 56]               0\n",
            "           Conv2d-25          [-1, 256, 56, 56]         590,080\n",
            "           Conv2d-26          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-27          [-1, 256, 56, 56]               0\n",
            "             ReLU-28          [-1, 256, 56, 56]               0\n",
            "           Conv2d-29          [-1, 256, 56, 56]         590,080\n",
            "           Conv2d-30          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-31          [-1, 256, 56, 56]               0\n",
            "             ReLU-32          [-1, 256, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]         590,080\n",
            "           Conv2d-34          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "             ReLU-36          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-37          [-1, 256, 28, 28]               0\n",
            "        MaxPool2d-38          [-1, 256, 28, 28]               0\n",
            "           Conv2d-39          [-1, 512, 28, 28]       1,180,160\n",
            "           Conv2d-40          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-41          [-1, 512, 28, 28]               0\n",
            "             ReLU-42          [-1, 512, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]       2,359,808\n",
            "           Conv2d-44          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-45          [-1, 512, 28, 28]               0\n",
            "             ReLU-46          [-1, 512, 28, 28]               0\n",
            "           Conv2d-47          [-1, 512, 28, 28]       2,359,808\n",
            "           Conv2d-48          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-49          [-1, 512, 28, 28]               0\n",
            "             ReLU-50          [-1, 512, 28, 28]               0\n",
            "           Conv2d-51          [-1, 512, 28, 28]       2,359,808\n",
            "           Conv2d-52          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-53          [-1, 512, 28, 28]               0\n",
            "             ReLU-54          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-55          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-56          [-1, 512, 14, 14]               0\n",
            "           Conv2d-57          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-58          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-59          [-1, 512, 14, 14]               0\n",
            "             ReLU-60          [-1, 512, 14, 14]               0\n",
            "           Conv2d-61          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-62          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-63          [-1, 512, 14, 14]               0\n",
            "             ReLU-64          [-1, 512, 14, 14]               0\n",
            "           Conv2d-65          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-66          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-67          [-1, 512, 14, 14]               0\n",
            "             ReLU-68          [-1, 512, 14, 14]               0\n",
            "           Conv2d-69          [-1, 512, 14, 14]       2,359,808\n",
            "           Conv2d-70          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-71          [-1, 512, 14, 14]               0\n",
            "             ReLU-72          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-73            [-1, 512, 7, 7]               0\n",
            "           Linear-74                 [-1, 1000]      25,089,000\n",
            "             ReLU-75                 [-1, 1000]               0\n",
            "          Dropout-76                 [-1, 1000]               0\n",
            "           Linear-77                    [-1, 2]           2,002\n",
            "          Sigmoid-78                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 65,139,770\n",
            "Trainable params: 65,139,770\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 476.43\n",
            "Params size (MB): 248.49\n",
            "Estimated Total Size (MB): 725.50\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8S_d4Rjy4Wl"
      },
      "source": [
        "def indices_to_one_hot(data, nb_classes):\n",
        "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
        "    targets = np.array(data).reshape(-1)\n",
        "    return np.eye(nb_classes)[targets]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "aGMkX2EGh5UC",
        "outputId": "12445b52-233d-48a8-d2fb-6123b46b28de"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "Tensor = torch.cuda.FloatTensor if device == 'cuda:0' else torch.FloatTensor\n",
        "\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        labels = torch.from_numpy(indices_to_one_hot(labels, 2))\n",
        "        inputs.to(device)\n",
        "        labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = vgg(inputs.type(Tensor))\n",
        "        #outputs = torch.tensor(output.argmax(dim=1), dtype = torch.float)\n",
        "\n",
        "        loss = criterion(output, labels.type(Tensor))\n",
        "        #loss.requres_grad = True\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2 == 1:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     2] loss: 1.075033\n",
            "[1,     4] loss: 0.717009\n",
            "[1,     6] loss: 0.606127\n",
            "[2,     2] loss: 0.479356\n",
            "[2,     4] loss: 0.439469\n",
            "[2,     6] loss: 0.440918\n",
            "[3,     2] loss: 0.345194\n",
            "[3,     4] loss: 0.234551\n",
            "[3,     6] loss: 0.235374\n",
            "[4,     2] loss: 0.229081\n",
            "[4,     4] loss: 0.177450\n",
            "[4,     6] loss: 0.116726\n",
            "[5,     2] loss: 0.110109\n",
            "[5,     4] loss: 0.118325\n",
            "[5,     6] loss: 0.114017\n",
            "[6,     2] loss: 0.137963\n",
            "[6,     4] loss: 0.101284\n",
            "[6,     6] loss: 0.091815\n",
            "[7,     2] loss: 0.081687\n",
            "[7,     4] loss: 0.053975\n",
            "[7,     6] loss: 0.053329\n",
            "[8,     2] loss: 0.026143\n",
            "[8,     4] loss: 0.032357\n",
            "[8,     6] loss: 0.027615\n",
            "[9,     2] loss: 0.023144\n",
            "[9,     4] loss: 0.031060\n",
            "[9,     6] loss: 0.016399\n",
            "[10,     2] loss: 0.023862\n",
            "[10,     4] loss: 0.020993\n",
            "[10,     6] loss: 0.016302\n",
            "[11,     2] loss: 0.016908\n",
            "[11,     4] loss: 0.010269\n",
            "[11,     6] loss: 0.008481\n",
            "[12,     2] loss: 0.016067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c436d2b11a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#loss.requres_grad = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jys7B3xylJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72103ba2-4979-47ee-ed36-a63793974773"
      },
      "source": [
        "# set the evaluation mode\n",
        "vgg.eval()\n",
        "\n",
        "# get the image from the dataloader\n",
        "#img, _ = next(iter(trainloader))\n",
        "#pred = vgg(img)\n",
        "\n",
        "img = cv2.imread(\"Pos_00100.tif\")\n",
        "img = cv2.resize(img, (224, 224) )\n",
        "img_tensor = torch.from_numpy(gray2rgb(img))\n",
        "#print(img_tensor.shape[0], img_tensor.shape[1], img_tensor.shape[2])\n",
        "img_tensor = img_tensor.permute(2, 0, 1)\n",
        "#print(img_tensor.shape[0], img_tensor.shape[1], img_tensor.shape[2])\n",
        "img_tensor = img_tensor.unsqueeze(0)\n",
        "#print(img_tensor.shape[0], img_tensor.shape[1], img_tensor.shape[2], img_tensor.shape[3])\n",
        "img_tensor.to(device)\n",
        "\n",
        "# get the most likely prediction of the model\n",
        "pred = vgg(img_tensor.type(Tensor))\n",
        "print(pred)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0033, 0.9987]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SO4kX5eyoOW"
      },
      "source": [
        "# get the gradient of the output with respect to the parameters of the model\n",
        "#Active = torch.nn.Sequential(*list(vgg.children())[:-1])\n",
        "#final_layer = list(vgg.children())[-1]\n",
        "\n",
        "pred[:, 1].backward()\n",
        "\n",
        "# pull the gradients out of the model\n",
        "gradients = vgg.get_activations_gradient()\n",
        "\n",
        "# pool the gradients across the channels\n",
        "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "\n",
        "# get the activations of the last convolutional layer\n",
        "activations = vgg.get_activations(img_tensor.type(Tensor)).detach()\n",
        "\n",
        "# weight the channels by corresponding gradients\n",
        "for i in range(512):\n",
        "    activations[:, i, :, :] *= pooled_gradients[i]\n",
        "    \n",
        "# average the channels of the activations\n",
        "heatmap = torch.mean(activations, dim=1).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrmMHHQXdlRV"
      },
      "source": [
        "# relu on top of the heatmap\n",
        "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
        "heatmap = np.maximum(heatmap.cpu(), 0)\n",
        "\n",
        "# normalize the heatmap\n",
        "heatmap /= torch.max(heatmap)\n",
        "\n",
        "# draw the heatmap\n",
        "plt.matshow(heatmap.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFLh4tSTy-GQ"
      },
      "source": [
        "print(img_tensor.shape[2], img_tensor.shape[3])\n",
        "heatmap = cv2.resize(np.float32(heatmap), (img_tensor.shape[2], img_tensor.shape[3]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "superimposed_img = heatmap * 0.3 + img\n",
        "cv2.imwrite('./map.jpg', superimposed_img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}